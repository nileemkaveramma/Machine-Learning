{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Demonstrate the Markov Decision Process (MDP) in Python"
      ],
      "metadata": {
        "id": "lZNMsy7jqjuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZD2Kw1DkTVM"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid world\n",
        "grid_size = 3\n",
        "goal_state = (2, 2)"
      ],
      "metadata": {
        "id": "cV6npSCtkyEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define states as positions in the grid (3x3)\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]"
      ],
      "metadata": {
        "id": "OtLa4OZ9lNlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define actions\n",
        "actions = ['up', 'down', 'left', 'right']"
      ],
      "metadata": {
        "id": "0BPyiW_hlPuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the rewards for each state-action pair\n",
        "rewards = {}\n",
        "for state in states:\n",
        "    if state == goal_state:\n",
        "        rewards[state] = 10  # Reward for reaching the goal\n",
        "    else:\n",
        "        rewards[state] = -1  # Penalty for every move"
      ],
      "metadata": {
        "id": "vHfhOaqAlSAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transition dynamics\n",
        "def next_state(state, action):\n",
        "    i, j = state\n",
        "    if action == 'up':\n",
        "        return (max(i-1, 0), j)  # Stay within bounds\n",
        "    elif action == 'down':\n",
        "        return (min(i+1, grid_size-1), j)  # Stay within bounds\n",
        "    elif action == 'left':\n",
        "        return (i, max(j-1, 0))  # Stay within bounds\n",
        "    elif action == 'right':\n",
        "        return (i, min(j+1, grid_size-1))  # Stay within bounds"
      ],
      "metadata": {
        "id": "KZDE-8QzlToQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transition probabilities (deterministic)\n",
        "def transition_probability(state, action, next_state_):\n",
        "    if next_state(state, action) == next_state_:\n",
        "        return 1.0  # Deterministic transition\n",
        "    else:\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "gjoc9QGRlV-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MDP model\n",
        "class MDP:\n",
        "    def __init__(self, states, actions, rewards, goal_state):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "        self.goal_state = goal_state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Get the next state\n",
        "        next_state_ = next_state(state, action)\n",
        "\n",
        "        # Get the reward for the next state\n",
        "        reward = self.rewards[next_state_]\n",
        "\n",
        "        # Return next state, reward\n",
        "        return next_state_, reward"
      ],
      "metadata": {
        "id": "6giBdUeHlZJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an MDP instance\n",
        "mdp = MDP(states, actions, rewards, goal_state)"
      ],
      "metadata": {
        "id": "JBhTcdjXlbmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a policy (random for simplicity)\n",
        "def random_policy(state):\n",
        "    return np.random.choice(actions)"
      ],
      "metadata": {
        "id": "N3iMx94glgVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the agent's journey through the grid\n",
        "def simulate_mdp(mdp, start_state, policy, max_steps=10):\n",
        "    state = start_state\n",
        "    total_reward = 0\n",
        "    print(f\"Starting state: {state}\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if state == mdp.goal_state:\n",
        "            print(f\"Reached goal state {mdp.goal_state} at step {step}\")\n",
        "            break\n",
        "\n",
        "        action = policy(state)\n",
        "        next_state_, reward = mdp.step(state, action)\n",
        "        total_reward += reward\n",
        "\n",
        "        print(f\"Step {step + 1}: State: {state}, Action: {action}, Next State: {next_state_}, Reward: {reward}\")\n",
        "        state = next_state_\n",
        "\n",
        "    print(f\"Total reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "cnmnB3FUljCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the simulation from the top-left corner (0, 0)\n",
        "simulate_mdp(mdp, start_state=(0, 0), policy=random_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXWmrfrLllsm",
        "outputId": "3c929259-5961-4bbd-8a5e-dc6e645fec21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting state: (0, 0)\n",
            "Step 1: State: (0, 0), Action: right, Next State: (0, 1), Reward: -1\n",
            "Step 2: State: (0, 1), Action: left, Next State: (0, 0), Reward: -1\n",
            "Step 3: State: (0, 0), Action: down, Next State: (1, 0), Reward: -1\n",
            "Step 4: State: (1, 0), Action: right, Next State: (1, 1), Reward: -1\n",
            "Step 5: State: (1, 1), Action: right, Next State: (1, 2), Reward: -1\n",
            "Step 6: State: (1, 2), Action: left, Next State: (1, 1), Reward: -1\n",
            "Step 7: State: (1, 1), Action: down, Next State: (2, 1), Reward: -1\n",
            "Step 8: State: (2, 1), Action: right, Next State: (2, 2), Reward: 10\n",
            "Reached goal state (2, 2) at step 8\n",
            "Total reward: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simulate_mdp(mdp, start_state=(0, 0), policy=random_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcCT9s3ylowr",
        "outputId": "8209be9b-09ab-4949-b031-809d8d0e0b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting state: (0, 0)\n",
            "Step 1: State: (0, 0), Action: down, Next State: (1, 0), Reward: -1\n",
            "Step 2: State: (1, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 3: State: (2, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 4: State: (2, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 5: State: (2, 0), Action: left, Next State: (2, 0), Reward: -1\n",
            "Step 6: State: (2, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 7: State: (2, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 8: State: (2, 0), Action: down, Next State: (2, 0), Reward: -1\n",
            "Step 9: State: (2, 0), Action: right, Next State: (2, 1), Reward: -1\n",
            "Step 10: State: (2, 1), Action: up, Next State: (1, 1), Reward: -1\n",
            "Total reward: -10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simulate_mdp(mdp, start_state=(0, 0), policy=random_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSJk0nhGlsGu",
        "outputId": "a38c3134-d883-4177-9d64-d1fc7d782d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting state: (0, 0)\n",
            "Step 1: State: (0, 0), Action: down, Next State: (1, 0), Reward: -1\n",
            "Step 2: State: (1, 0), Action: up, Next State: (0, 0), Reward: -1\n",
            "Step 3: State: (0, 0), Action: left, Next State: (0, 0), Reward: -1\n",
            "Step 4: State: (0, 0), Action: left, Next State: (0, 0), Reward: -1\n",
            "Step 5: State: (0, 0), Action: left, Next State: (0, 0), Reward: -1\n",
            "Step 6: State: (0, 0), Action: up, Next State: (0, 0), Reward: -1\n",
            "Step 7: State: (0, 0), Action: up, Next State: (0, 0), Reward: -1\n",
            "Step 8: State: (0, 0), Action: down, Next State: (1, 0), Reward: -1\n",
            "Step 9: State: (1, 0), Action: right, Next State: (1, 1), Reward: -1\n",
            "Step 10: State: (1, 1), Action: up, Next State: (0, 1), Reward: -1\n",
            "Total reward: -10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJeCe0ralvcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}